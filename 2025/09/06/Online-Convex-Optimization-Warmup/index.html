

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Yuheng Zhao">
  <meta name="keywords" content="">
  
    <meta name="description" content="Learn to trade-off">
<meta property="og:type" content="article">
<meta property="og:title" content="Online Convex Optimization Warmup">
<meta property="og:url" content="http://example.com/2025/09/06/Online-Convex-Optimization-Warmup/index.html">
<meta property="og:site_name" content="Zrkc&#39;s Blog">
<meta property="og:description" content="Learn to trade-off">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-09-06T03:01:07.000Z">
<meta property="article:modified_time" content="2025-09-07T11:15:41.501Z">
<meta property="article:author" content="Yuheng Zhao">
<meta property="article:tag" content="OCO">
<meta property="article:tag" content="Tutorial">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>Online Convex Optimization Warmup - Zrkc&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Zrkc (Yuheng Zhao)&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/RadioTelescope.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Online Convex Optimization Warmup"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-09-06 11:01" pubdate>
          September 6, 2025 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.2k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          60 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Online Convex Optimization Warmup</h1>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>本文是为<a target="_blank" rel="noopener" href="https://www.pengzhao-ml.com/course/AOpt2025fall/">高级优化课程</a>的学生在正式学习在线优化之前准备的。具体来说，本文介绍了在线优化的一个经典凸函数 setting，展示经典的在线梯度下降算法，并推导了理论保障。<br>本文的目的是希望读者能在正式学习在线优化之前，先对其分析过程有个大致认识，以免陷于数学证明之中失去了最宝贵的兴趣。为照顾逻辑表达，本文行文叙述和理论推导可能不会十分严谨；欢迎评论！:)</p>
</blockquote>
<h2 id="0-Introduction-Why-Online-Optimization"><a href="#0-Introduction-Why-Online-Optimization" class="headerlink" title="0. Introduction: Why Online Optimization?"></a>0. Introduction: Why Online Optimization?</h2><h3 id="0-1-Offline-Optimization"><a href="#0-1-Offline-Optimization" class="headerlink" title="0.1 (Offline) Optimization"></a>0.1 (Offline) Optimization</h3><p>本文从传统的优化角度引入。大多数人接触优化是从随机梯度下降（Stochastic Gradient Descent, SGD）开始的，再后来就是调包创建优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs py">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure>
<p>并训练（优化）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero _grad()<br>    output = model(<span class="hljs-built_in">input</span>)<br>    loss = loss _fn(output, target)<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure>
<p>截止目前（2025年），最常用的优化器是 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">Adam</a> 以及其变体 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.05101">AdamW</a>。优化器还有很多，但是为什么现在大家几乎默认使用 Adam 呢？一个很有意思的<a target="_blank" rel="noopener" href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/">观点</a>是：Adam 很适合优化若干年前的神经网络，因此神经网络近些年都在朝着能够继续使得 Adam 有效甚至越来越有效的方向进化。至于在此期间还有其他可能的新架构，往往因为诞生之初难以被<em>默认</em>的 Adam 优化而被忽视了。如果事实真就如此，我们能否从理论上说明这个现象，并指导下一个优化器（或者更可能说是，下一对“模型-优化器”）的设计呢？</p>
<p>我们需要回到优化的数学形式化描述上。例如，对于“feature-target”对的数据集 $\{(\mathbf{z} _n,\mathbf{y} _n)\} _{n&#x3D;1}^N$，我们定义参数为 $\mathbf{w}$ 的模型为函数 $\mathcal{M}(\cdot;\mathbf{w}):\mathbf{z}\mapsto\mathcal{M}(\mathbf{z};\mathbf{w})$。定义单值的损失函数 $l(\cdot,\cdot)$ 用以衡量模型输出 $\mathcal{M}(\mathbf{z} _n;\mathbf{w})$ 和真实目标 $\mathbf{y} _n$ 之间的差距。那么优化问题可以写成：<br>$$<br>\begin{equation}<br>    \min _{\mathbf{w}}\quad  \frac{1}{N} \sum _{n&#x3D;1}^N l(\mathcal{M}(\mathbf{z} _n;\mathbf{w}),\mathbf{y} _n). \tag{1}<br>\end{equation}<br>$$<br>我们假设数据集是从某个分布 $\mathcal{D}$ 上独立同分布采样得到的，从而令优化问题为：<br>$$<br>\begin{equation}<br>    \min _{\mathbf{w}}\quad  \mathbb{E} _{(\mathbf{z,y})\sim\mathcal{D}}\left[ l(\mathcal{M}(\mathbf{z};\mathbf{w}),\mathbf{y})\right]. \tag{2}<br>\end{equation}<br>$$<br>上述问题可以更一般化：我们定义优化目标函数为 $\mathcal{L}(\mathbf{w}):\mathbf{w}\mapsto \mathbb{R}$，并假设它有一个随机 Oracle $\ell(\mathbf{w})$，满足 $\mathbb{E}[\ell(\mathbf{w})\mid\mathbf{w}] &#x3D; \mathcal{L}(\mathbf{w})$。例如，对于某个样本 batch $\mathcal{B}&#x3D;\{(\mathbf{z} _n,\mathbf{y} _n)\}$，对应的损失 $\frac{1}{|\mathcal{B}|}\sum _{(\mathbf{z} _n,\mathbf{y} _n)\in\mathcal{B}}l(\mathcal{M}(\mathbf{z} _n;\mathbf{w}),\mathbf{y} _n)$ 可以看作是对随机 Oracle 的一次访问 $\ell(\mathbf{w})$。我们设定 $\mathbf{w}$ 属于某个参数空间的集合 $\mathcal{W}\subseteq\mathbb{R}^d$，最终优化问题为：<br>$$<br>\begin{equation}<br>    \min _{\mathbf{w}\in\mathcal{W}}\quad \mathcal{L}(\mathbf{w}). \tag{3}<br>\end{equation}<br>$$<br>这个优化问题也被称为“随机优化（Stochastic Optimization）”。我们将最优参数记作 $\mathbf{w}^\star\triangleq \arg\min _{\mathbf{w}\in\mathcal{W}}\mathcal{L}(\mathbf{w})$。</p>
<p>这里我们研究优化过程的收敛性质。具体来说，假设优化过程中我们访问了 $T$ 次随机 Oracle（例如，计算了 $T$ 个 batch），优化器最终给出的模型参数为 $\mathbf{w}^\dagger$。考虑 $\mathbf{w}^\dagger$ 的损失和最优参数 $\mathbf{w}^\star$ 的差距（optimality gap），（渐进意义下）是否存在某个关于 $T$ 的函数作为上界。例如，关于 Oracle 随机性的期望上界：<div id="def-convergence"></div><br>$$<br>\begin{equation}<br>    \mathbb{E}\left[ \mathcal{L}(\mathbf{w}^\dagger) - \mathcal{L}(\mathbf{w}^\star) \right] \le \epsilon(T), \tag{4}<br>\end{equation}<br>$$<br>或者高概率上界，即以至少 $(1-\delta)$ 的概率：<br>$$<br>\begin{equation}<br>\mathcal{L}(\mathbf{w}^\dagger) - \mathcal{L}(\mathbf{w}^\star) \le \epsilon(T,\delta). \tag{5}<br>\end{equation}<br>$$<br>正常来说我们期望 $\epsilon(T)$ 是随着 $T$ 递减的，常见的形式比如 $\mathcal{O}\big(\frac{1}{\sqrt{T}}\big),\mathcal{O}\big(\frac{1}{T}\big)$ 等。研究收敛性质还有很多角度，例如梯度范数 $\Vert\nabla \mathcal{L}(\mathbf{w})\Vert$ 的收敛；或者对称地，当 gap 达到目标值 $\epsilon$（例如 $\mathbb{E}[\mathcal{L}(\mathbf{w}^\dagger) - \mathcal{L}(\mathbf{w}^\star)]\le \epsilon$）时所需要的 Oracle 访问次数上界 $T(\epsilon)$。</p>
<h3 id="0-2-Offline-to-Online"><a href="#0-2-Offline-to-Online" class="headerlink" title="0.2 Offline to Online"></a>0.2 Offline to Online</h3><p>讲到这里，和我们要介绍的在线优化是什么关系呢？我们考虑模型的两个阶段：离线训练和在线部署。离线阶段，模型对着固定的数据集训练；而在线部署后，模型面对流式到来的数据，也可能有优化参数以适应新数据的需要，这方面的例子比如后训练（或者多次后训练）、持续学习等。为了对比和区分，我们将前者的优化过程（例如上文提到的随机优化）称为“离线优化”，而将后者称为“在线优化（Online Optimization）”。你可能会问，随机优化里一个个 batch 的数据，不也是流式的吗？——确实如此，这也是在线学习方法可以很自然地迁移到离线优化的一个原因——但离线和在线更重要的区别在于：<strong>数据分布是否会发出变化</strong>。</p>
<p>对于离线优化而言，优化过程中 Oracle 的分布始终不变，就比如在随机优化中，每个 batch 都是<strong>独立同分布</strong>的。事实上，当数据集被扩充、或者进入在线部署阶段后，数据分布很可能会发生变化，始终独立同分布的条件就不成立了。此时，优化问题从“离线”的一成不变转为“在线”的持续变化，“在线优化”也就有了意义。</p>
<p>等等，在上一小节我们不是说好了要研究优化器的理论吗？这饼给我画哪去了？事实上，正如上所述，在线优化的方法可以很自然地用于离线优化尤其是随机优化，原因也很简单：在线学习研究的是分布可以发生变化，它的一个特殊情况就是分布不变的随机优化。而从理论上，也有一个很简单直接的转化，称为<a target="_blank" rel="noopener" href="https://parameterfree.com/2019/09/17/more-online-to-batch-examples-and-strong-convexity/">“online-to-batch conversion”</a>，它的意思是式 <a href="#def-convergence">$(4)$</a> 中的 optimality gap 可以被平均意义下的在线学习优化目标——遗憾（Regret）——给 upper bound 住，从而我们的问题转变为了为 Regret 提供理论保障。在线优化所属的在线学习（Online Learning），最早由<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Gabor-Lugosi/publication/220690817_Prediction_Learning_and_Games/links/0912f50eae7fc7be04000000/Prediction-Learning-and-Games.pdf">《Prediction, learning, and games》</a>一书从博弈论的角度所形式化，并给出了 Regret 概念作为理论指标。</p>
<p>除此之外，作为研究分布变化情况下的优化问题，在线优化也具备更广泛的意义和应用场景。毕竟没有一成不变的环境，为了得到有适应能力的模型，在线优化也算是起了一个开头了。</p>
<h2 id="1-Problem-Formulation"><a href="#1-Problem-Formulation" class="headerlink" title="1. Problem Formulation"></a>1. Problem Formulation</h2><p>在线凸优化是在线学习（Online Learning）的一个 case。在线学习简单来说就是设计算法使得能够随着环境的变化不断更新自身模型适应它，例如：与环境产生交互、数据按批次处理等问题。</p>
<p>我们将它抽象成学习者与环境的博弈，即如下范式：</p>
<p><strong>在线学习.</strong><br>已知决策可行域 $\mathcal{X}\subseteq\mathbb{R}^d$。在第 $t&#x3D;1,2,\dots$ 轮：</p>
<ol>
<li>学习者选择决策 $\mathbf{x} _t\in\mathcal{X}\subseteq \mathbb{R}^d$；</li>
<li>环境给出在线函数 $f _t:\mathcal{X}\to\mathbb{R}$；</li>
<li>学习者受到 $f _t(\mathbf{x} _t)$ 的损失，观测到某些 $f _t$ 的信息并更新决策。</li>
</ol>
<p>在线凸优化（Online Convex Optimization, OCO）研究可行域 $\mathcal{X}$ 为凸集且在线函数 $f _t$ 均为凸函数的情况。</p>
<h3 id="1-1-Performance-Measure"><a href="#1-1-Performance-Measure" class="headerlink" title="1.1 Performance Measure"></a>1.1 Performance Measure</h3><p>最直接地，我们希望最小化累积损失<br>$$<br>\def \x {\mathbf{x}}<br>\sum _{t&#x3D;1}^T f _t(\x _t).<br>$$</p>
<p>但这个指标很不公平、导致难以下手，因为环境可以对抗性地根据 $\mathbf{x} _t$ 给出 $f _t$。类似超额风险（模型损失与最优模型损失之差）引入最优决策作为 comparator 限制优化目标的难度，我们采用称为“遗憾”（regret）的度量指标，定义为：</p>
<div id="regret"></div>

<p>$$<br>\def \x {\mathbf{x}}<br>Reg _T \triangleq \sum _{t&#x3D;1}^T f _t(\x _t) - \min _{\x\in\mathcal{X}}\sum _{t&#x3D;1}^T f _t(\x). \tag{1}<br>$$</p>
<p>我们简记 $\mathbf{x} _\star\triangleq \argmin _{\mathbf{x}\in\mathcal{X}}\sum _{t&#x3D;1}^T f _t(\mathbf{x} _t)$。学习者的目标是最小化遗憾，由于环境的决策此时也会影响 $\mathbf{x} _\star$ 的累积损失，所以这是一个合理的博弈。</p>
<p>在本文中，我们还同时关注两个 OCO 中比较常见的假设：可行域有界假设，和梯度有界假设。</p>
<ul>
<li><strong>可行域有界假设.</strong> 对任意 $\mathbf{x,y}\in\mathcal{X}$，有 $|\mathbf{x-y}| _2\le D$.  </li>
<li><strong>梯度有界假设.</strong> 对任意 $\mathbf{x}\in\mathcal{X}$ 和 $t\in[T]$，有 $|\nabla f _t(\mathbf{x})| _2\le G$。即在线函数是 $G$-Lipschitz 的。</li>
</ul>
<p>下文简记 $\ell _2$-范数 $|\cdot| _2$ 为 $|\cdot|$。引入这两个假设最直接的作用就是简化了算法设计和证明。这两个假设也很基本，从量纲上来说分别对应了距离和斜率，乘起来就是损失。当然也有很多工作在研究不依赖这两个假设或者对应参数的算法，至今仍有很多有待研究的空间。</p>
<h3 id="1-2-A-Trivial-Example"><a href="#1-2-A-Trivial-Example" class="headerlink" title="1.2 A Trivial Example"></a>1.2 A Trivial Example</h3><p>下文中，我们要做的就是设计算法，并证明 regret 有上界保证——通常用渐进复杂度上界表示——并且至少得是关于 $T$ 亚线性的。为什么会这么考虑呢？首先，亚线性意味着平均意义下相对于最优决策的损失之差会趋于零：$\lim _{T\to\infty}\frac{Reg _T}{T}&#x3D;0$，从而累积损失趋近于全局最优决策 $\mathbf{x} _\star$ 的累计损失。其次，一个关于 $T$ 线性的上界是 trivial 的。最简单地，根据微分中值定理：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \c {\mathbf{c}}<br>\begin{align*}<br>Reg _T &amp;&#x3D; \sum _{t&#x3D;1}^T f _t(\x _t) - \sum _{t&#x3D;1}^T f _t(\xs) &#x3D; \sum _{t&#x3D;1}^T \langle \nabla f _t(\c _t), \x _t - \xs \rangle \qquad(\c _t\in[\x _t,\xs]) \<br>&amp;\le \sum _{t&#x3D;1}^T | \nabla f _t(\c _t) | |\x _t - \xs| \le GDT.<br>\end{align*}<br>$$<br>其中第一个不等号使用了 Hölder 不等式。我们可以稍微观察一下这个上界：$GD$ 是最大“斜率”乘最大“距离”，也就是每轮可能的最大损失，这个损失累积了 $T$ 轮，所以 $GDT$ 就是 regret 可能的最大值，是非常 trivial 的。</p>
<p>推导上界一定得是一个有意义的上界✍️。在后文中，我们会最终得到一个亚线性的 $\mathcal{O}(GD\sqrt{T})$ 上界，并证明（在某种意义上）无法做到更优。</p>
<p>本节的最后，也是在很多经典文献中证明 regret 上界的第一步，就是做线性化：即利用 $f _t(\mathbf{x})$ 为凸函数这一假设：</p>
<div id="regret-linearized"></div>

<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>Reg _T &#x3D; \sum _{t&#x3D;1}^T f _t(\x _t) - \sum _{t&#x3D;1}^T f _t(\xs) \le \sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs \rangle. \tag{2}<br>$$</p>
<p>相比刚才的微分中值定理，好处是我们不需要知道那个中值点 $\mathbf{c} _t$，坏处也很显然，假设函数为凸本身就局限了它的应用。无论如何，接下来可以设计我们的第一个在线凸优化算法了！</p>
<h2 id="2-Online-Gradient-Descent"><a href="#2-Online-Gradient-Descent" class="headerlink" title="2. Online Gradient Descent"></a>2. Online Gradient Descent</h2><h3 id="2-1-Why-Online-Gradient-Descent"><a href="#2-1-Why-Online-Gradient-Descent" class="headerlink" title="2.1 Why Online Gradient Descent?"></a>2.1 Why Online Gradient Descent?</h3><p>OCO 问题最经典的算法框架之一是在线梯度下降（Online Gradient Descent, OGD），即<br>$$<br>\def \x {\mathbf{x}}<br>\def \xt {\widetilde{\x}}<br>\xt _{t+1} &#x3D; \x _t - \eta _t \nabla f _t(\x _t),\quad \x _{t+1} &#x3D; \Pi _{\mathcal{X}}[ \xt _{t+1} ],<br>$$<br>其中 $\eta _t&gt;0$ 是步长，$\Pi _{\mathcal{X}}[\cdot]$ 是向凸集 $\mathcal{X}$ 的投影操作。可能你立即会问了，为什么：一定得从上一步做更新而不是上上步甚至随便一个点？从博弈过程来看，$f _t$ 和 $f _{t+1}$ 似乎没什么联系，那么从 $\mathbf{x} _t$ 拿着 $f _t$ 在该点的梯度去更新得到 $\mathbf{x} _{t+1}$，对于优化 $f _{t+1}$ 能有多大帮助呢？</p>
<p>这里主要解释直观上为什么 OGD 能 work。我们先抛开 OCO 的语义，从数学角度看这个问题。在后文中我们会经常见到一类数学结构，它们看起来非常类似余弦定理：<br>$$<br>\def \a {\mathbf{a}}<br>\def \b {\mathbf{b}}<br>\def \c {\mathbf{c}}<br>2\langle \a - \b, \a - \c \rangle &#x3D; |\a - \b|^2 + |\a - \c|^2 - |\b - \c|^2.<br>$$</p>
<p>我们观察式 <a href="#regret-linearized">$(2)$</a> 线性化后的 regret：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs\rangle,<br>$$</p>
<p>同样是内积式。如果需要将 regret 和余弦定理联系起来，一个思路是观察到向余弦定理代入：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>2\langle \x _t - \x _{t+1}, \x _t - \xs \rangle &#x3D; |\x _t - \x _{t+1}|^2 + |\x _t - \xs|^2 - |\x _{t+1} - \xs|^2.<br>$$</p>
<p>这个内积式目前还不依赖任何 $\mathbf{x} _t$ 和 $\mathbf{x} _{t+1}$ 之间的关系，也就是适用于任意序列 $\{\mathbf{x} _t\} _{t&#x3D;1}^T$。对 $t&#x3D;1$ 到 $T$ 求和，会发现它可以<strong>错位相消</strong>（telescoping）：</p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>2\sum _{t&#x3D;1}^T \langle \x _t - \x _{t+1}, \x _t - \xs \rangle &#x3D; \sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2 + |\x _1 - \xs|^2 - |\x _{T+1} - \xs|^2.<br>$$</p>
<p>能做到错位相消是一个我们比较满意的结构。为了让它和 regret 沾上边，对比二者形式我们发现最好让 $\mathbf{x} _t-\mathbf{x} _{t+1}$ 和 $\nabla f _t(\mathbf{x} _t)$ 对应。当然我们还可以引入步长 $\eta$ 作为算法的参数。如果不考虑投影操作，最简单的设计就是 $\mathbf{x} _t-\mathbf{x} _{t+1}&#x3D;\eta\nabla f _t(\mathbf{x} _t)$，它已经很类似在线梯度下降的更新式了。代入 regret 可得：<div id="regret-telescoping"></div></p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>Reg _T &amp;\le \sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs \rangle &#x3D; \frac{1}{\eta}\sum _{t&#x3D;1}^T \langle \x _t - \x _{t+1}, \x _t - \xs \rangle \<br>&amp;&#x3D; \frac{1}{2\eta}\left(\sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2 + |\x _1 - \xs|^2 - |\x _{T+1} - \xs|^2. \right) \<br>&amp;&#x3D; \frac{\eta}{2}\sum _{t&#x3D;1}^T |\nabla f _t(\x _t)|^2 + \frac{1}{2\eta}\left(|\x _1 - \xs|^2 - |\x _{T+1} - \xs|^2\right). \tag{3}<br>\end{align*}<br>$$</p>
<p>令人高兴的是，我们可以进一步利用 $\eta$ 在上式做 trade-off！可以大概预想到，通过合理设置 $\eta$，我们能得到一个亚线性，具体而言，$\mathcal{O}(\sqrt{T})$ 的上界。</p>
<p>在阅读下一小节之前，你可能会有一些自己的想法。你可以试试自己推导看看，这会很快帮助你形成一个比较合理的认知 :)</p>
<h3 id="2-2-Some-Questions"><a href="#2-2-Some-Questions" class="headerlink" title="2.2 Some Questions"></a>2.2 Some Questions</h3><p>到这里，不知道你会不会问：为什么不做“梯度上升”呢？从分析上看似乎也有些道理：上面的余弦定理是两个正项和一个负项，梯度上升意味着使用余弦定理取负，变成两个负项一个正项，而且依然保留 telescoping 的结构！但是，不能这么做的原因在于，那个原本为负、现在为正的项，最终会变得难以控制地大。</p>
<p>例如，令 $\mathbf{x} _t-\mathbf{x} _{t+1}&#x3D;-\eta\nabla f _t(\mathbf{x} _t)$，代入：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\eta\langle \nabla f _t(\x _t), \x _t - \xs\rangle &#x3D; -<br>\langle \x _t - \x _{t+1}, \x _t - \xs \rangle &#x3D; \frac{1}{2}\Big(|\x _{t+1} - \xs|^2 - |\x _t - \xs|^2 - |\x _t - \x _{t+1}|^2\Big).<br>$$</p>
<p>对 $t&#x3D;1$ 到 $T$ 求和（注意我们忽略了投影）：<div id="gradient-ascent"></div></p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>Reg _T \le \frac{1}{2\eta}\left(|\x _{T+1} - \xs|^2 - |\x _1 - \xs|^2 - \sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2\right) . \tag{4}<br>$$</p>
<p>看起来只有唯一的正项 $|\mathbf{x} _{T+1}-\mathbf{x} _\star|^2$，莫非这是一个好事？但这还没完。式 <a href="#regret-telescoping">$(3)$</a> 中的正项 $|\mathbf{x} _1 - \mathbf{x} _\star|^2$，通常来说，我们会认为它是一个和初始点选择有关的常数；而式 <a href="#gradient-ascent">$(4)$</a> 中的正项 $|\mathbf{x} _{T+1}-\mathbf{x} _\star|^2$ 就是一个和算法运行过程相关的量了。因为推导忽略了投影，相当于令 $\mathcal{X}&#x3D;\mathbb{R}^d$，所以此时可行域有界假设不成立；万一 $|\mathbf{x} _{T+1}-\mathbf{x} _\star|^2$ 随着 $T$ 增长速度是线性的呢？这也体现了证明上界的一个基本原则：虽然证明过程中产生了很多大等于 regret 的式子，但是只有当这个式子有普适意义时（例如，和算法运行过程量无关，体现为某些全局参数的函数），才能够和其他算法对比。下一步，我们顶多利用 Cauchy–Schwarz 不等式：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>&amp; |\x _{T+1} - \xs|^2 &#x3D; |\x _{T+1}-\x _T+\x _T - \x _{T-1} + \cdots + \x _2 - \x _1 + \x _1 - \xs|^2 \<br>\le{} &amp; (T+1)\left(|\x _{T+1}-\x _T|^2 + |\x _T - \x _{T-1}|^2 + \cdots + |\x _2 - \x _1|^2 + |\x _1 - \xs|^2 \right),<br>\end{align*}<br>$$</p>
<p>代入式 <a href="#gradient-ascent">$(4)$</a> 得到：<div id="gradient-ascent-regret"></div></p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>Reg _T &amp;\le \frac{T}{2\eta}\left( |\x _1 - \xs|^2 + \sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2 \right) &#x3D; \frac{T}{2}\left( \frac{1}{\eta}|\x _1 - \xs|^2 + \eta\sum _{t&#x3D;1}^T |\nabla f _t(\x _t)|^2 \right). \tag{5}<br>\end{align*}<br>$$</p>
<p>很显然，和梯度下降的式 <a href="#regret-telescoping">$(3)$</a> 相比，式 <a href="#gradient-ascent-regret">$(5)$</a> 中 $\eta$ 最多只能在括号里头做 trade-off，而括号外还有一个线性的 $T$，高下立判！</p>
<p>总而言之，在线梯度下降能够 work 就是因为它合理地利用了余弦定理形式的 telescoping 结构。你可能还会问：为什么得从上一步而不是上上步或者其他呢？从上面的推导可以看出，从上一步只是为了方便做 telescoping 的衔接。事实上，不一定得从上一步开始，只要你能构造出一条（或几条）的链条就行了，比如：<br>$$<br>\def \x {\mathbf{x}}<br>\x _1 \underset{-\nabla f _1(\x _1)}{\longrightarrow} \x _3 \underset{-\nabla f _3(\x _3)}{\longrightarrow} \cdots,\quad \x _2 \underset{-\nabla f _2(\x _2)}{\longrightarrow} \x _4 \underset{-\nabla f _4(\x _4)}{\longrightarrow} \cdots.<br>$$<br>更夸张地，你还可以：<br>$$<br>\def \x {\mathbf{x}}<br>\x _1 \underset{-\nabla f _1(\x _1)}{\longrightarrow} \x _T ,\quad \x _2 \underset{-\nabla f _2(\x _2)}{\longrightarrow} \x _3 \underset{-\nabla f _3(\x _3)}{\longrightarrow} \cdots \underset{-\nabla f _{T-2}(\x _{T-2})}{\longrightarrow} \x _{T-1}.<br>$$</p>
<p>不过其实就相当于把在线函数分为两组，跑两个 OGD 算法，本质上还是脱离不开“从上一步更新”的框架。能做成 telescoping 的关键还在于要从 $\mathbf{x} _t$ 用这点的梯度信息 $\nabla f _t(\mathbf{x} _t)$ 更新，剩下的就是一些变体了。</p>
<p>下一节中，我们对在线梯度下降进行正式的理论推导，并得到第一个亚线性的上界。</p>
<h2 id="3-Regret-Analysis"><a href="#3-Regret-Analysis" class="headerlink" title="3. Regret Analysis"></a>3. Regret Analysis</h2><h3 id="3-1-Bias-Variance"><a href="#3-1-Bias-Variance" class="headerlink" title="3.1 Bias-Variance"></a>3.1 Bias-Variance</h3><p>回到在线梯度下降：<div id="gradient-descent"></div></p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xt {\widetilde{\x}}<br>\xt _{t+1} &#x3D; \x _t - \eta _t \nabla f _t(\x _t),\quad \x _{t+1} &#x3D; \Pi _{\mathcal{X}}[ \xt _{t+1} ]. \tag{6}<br>$$</p>
<p>虽然看起来多了一步投影，但是和上面的推导基本一样：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \xt {\widetilde{\x}}<br>\begin{align*}<br>Reg _T &amp;\le \sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs\rangle &#x3D; \sum _{t&#x3D;1}^T \frac{1}{\eta _t}\langle \x _t - \xt _{t+1}, \x _t - \xs\rangle \<br>&amp;&#x3D; \sum _{t&#x3D;1}^T \frac{1}{2\eta _t} \Big(| \x _t - \xt _{t+1} |^2 + |\x _t - \xs|^2 - |\xt _{t+1} - \xs|^2 \Big).<br>\end{align*}<br>$$</p>
<p>为了 telescoping，我们利用投影的性质（毕达哥拉斯定理）做一步放缩：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \y {\mathbf{y}}<br>\def \xt {\widetilde{\x}}<br>\x &#x3D; \Pi _{\mathcal{X}}[ \xt ],\quad\implies\quad \forall \y\in\mathcal{X},|\x-\y|\le |\xt - \y|.<br>$$</p>
<p>于是上式：<div id="gradient-descent-regret"></div></p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \xt {\widetilde{\x}}<br>\begin{align*}<br>Reg _T &amp;\le \sum _{t&#x3D;1}^T \frac{1}{2\eta _t} \Big(| \x _t - \xt _{t+1} |^2 + |\x _t - \xs|^2 - |\x _{t+1} - \xs|^2 \Big) \<br>&amp;&#x3D; \frac{1}{2}\sum _{t&#x3D;1}^T \eta _t| \nabla f _t(\x _t) |^2 + \sum _{t&#x3D;1}^T \frac{1}{2\eta _t}\Big(|\x _t - \xs|^2 - |\x _{t+1} - \xs|^2 \Big). \tag{7}<br>\end{align*}<br>$$</p>
<p>该式子可以看出有两部分：第一部分梯度范数累积正比于步长，第二部分 telescoping 反比于步长。大致理解来看，我们可以认为第一部分是算法的 variance；第二部分是算法的 bias（这个含义在下一篇文章中更加明确地拆解出来，挖坑）。它也体现了一个在线学习算法永远在做的一件事：在探索与利用之间做平衡——每一步更新既要根据新信息做及时的更新，也要充分利用已有信息——平衡的关键就在于步长的设计。对于在线梯度下降来说，从上一步用梯度走多远，就是探索与利用的过程。这个思想在很多领域都有所体现。</p>
<p>在下一篇文章中，我们也会尝试更加显式地拆分出 trade-off 的两部分。这个拆分也将引出一类更加具有自适应能力的在线学习算法，并和更广泛的领域产生联系。</p>
<p>总之，我们先尝试设计步长以 trade-off。</p>
<h3 id="3-2-Step-size-Design"><a href="#3-2-Step-size-Design" class="headerlink" title="3.2 Step-size Design"></a>3.2 Step-size Design</h3><p>先考虑简单情况。如果 $\eta _t\equiv \eta$，则式 <a href="#gradient-descent-regret">$(7)$</a>：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>Reg _T \le \frac{\eta}{2} \sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2 + \frac{1}{2\eta}|\x _1 - \xs|^2 \le \frac{\eta G^2 T}{2} + \frac{D^2}{2\eta} &#x3D; GD\sqrt{T}. \tag{8}<br>$$<br>其中我们令 $\eta&#x3D;\frac{D}{G\sqrt{T}}$。</p>
<p>这里还可以考虑变步长 $\eta _t$，得到一个更有意思的设计。具体需要利用 Self-confident Tuning Lemma 这一数学工具，假设 $a  _1 ,\dots, a  _T$ 是非负实数，则：<br>$$<br>\sqrt{\sum _{t&#x3D;1}^T a _t} \le \sum _{t&#x3D;1}^T\frac{a _t}{\sqrt{\sum _{s&#x3D;1}^t a _s}}\le 2\sqrt{\sum _{t&#x3D;1}^T a _t}.<br>$$<br>现在参照上述引理，我们使用变步长并定义为 $\eta _t&#x3D;\frac{\alpha}{\sqrt{\sum _{s&#x3D;1}^t|\nabla f _s(\mathbf{x} _s)|^2}}$（其也称为 “adaptive step-size”，$\alpha&gt;0$ 为待定常数）。回到式 <a href="#gradient-descent-regret">$(7)$</a>：<div id="gradient-descent-adaptive-bound"></div></p>
<p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>Reg _T &amp;\le \frac{1}{2}\sum _{t&#x3D;1}^T \eta _t| \nabla f _t(\x _t) |^2 + \sum _{t&#x3D;2}^T \left(\frac{1}{2\eta _t} - \frac{1}{2\eta _{t-1}}\right) |\x _t - \xs|^2 + \frac{1}{2\eta _1}|\x _1 - \xs|^2 \<br>&amp;\le \frac{1}{2}\sum _{t&#x3D;1}^T \eta _t| \nabla f _t(\x _t) |^2 + \frac{D^2}{2\eta _T} &#x3D; \frac{\alpha}{2}\sum _{t&#x3D;1}^T \frac{|\nabla f _t(\x _t)|^2}{\sqrt{\sum _{s&#x3D;1}^t|\nabla f _s(\x _s)|^2}} + \frac{D^2\sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2}}{2\alpha} \<br>&amp;\le \left(\alpha + \frac{D^2}{2\alpha}\right) \sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2} &#x3D; \mathcal{O}\left( D\sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2}\right) &#x3D; \mathcal{O}(GD\sqrt{T}), \tag{9}<br>\end{align*}<br>$$<br>其中我们令 $\alpha&#x3D;D$。相较于 $\eta&#x3D;\frac{D}{G\sqrt{T}}$，这里变步长的好处是算法只需要事先知道 $D$——由于投影操作显式地需要可行域，这一般是已知的。不需要 $T$ 意味着算法可以运行任意轮数，这也称为 “anytime” 算法。</p>
<h2 id="4-Lower-Bound"><a href="#4-Lower-Bound" class="headerlink" title="4. Lower Bound"></a>4. Lower Bound</h2><p>回顾证明，我们主要在两个地方使用了不等号：第一处是线性化，第二处是利用投影性质做的放缩。这两个放缩会不会导致我们的上界 $\mathcal{O}(GD\sqrt{T})$ 太松了呢？本节将会说明，对于“<strong>最坏情况</strong>”，它是紧的。</p>
<p>由于我们证明的上界对于所有 OCO setting 都成立，为了说明没有更优的算法或者分析方法能够进一步从阶上改进这个上界，就需要证明对于任意算法，总存在一个最坏情况使得该算法具有 $\Omega(GD\sqrt{T})$ 的 regret 下界。也就是说，我们尝试证明：</p>
<p>$$<br>\def \x {\mathbf{x}}<br>\inf _{\text{algorithm}} \sup _{\text{environment}} Reg _T \ge \Omega(GD\sqrt{T}). \tag{10}<br>$$</p>
<p>本文采用的证明思路为，当确定可行域 $\mathcal{X}$ 和在线函数集 $\mathcal{F}$ 后，我们可以直接求解以 regret 为博弈指标的 minimax 值：<br>$$<br>\def \x {\mathbf{x}}<br>\def \X {\mathcal{X}}<br>\def \F {\mathcal{F}}<br>\mathcal{G} _T\triangleq \inf _{\x _1\in\X} \sup _{f _1\in\F} \inf _{\x _2\in\X} \sup _{f _2\in\F} \cdots \inf _{\x _T\in\X} \sup _{f _T\in\F} \left(\sum _{t&#x3D;1}^T f _t(\x _t) - \min _{\x\in\X}\sum _{t&#x3D;1}^T f _t(\x) \right). \tag{11}<br>$$</p>
<p>所以我们只需要构造出一组 $(\mathcal{X,F})$，并证明此时 $\mathcal{G} _T\ge \Omega(GD\sqrt{T})$ 即可。我们考虑一个简单（但是依然不容易）的 case：令 $\mathcal{X}&#x3D;\{\mathbf{x}:|\mathbf{x}|\le D\}$，以及 $\mathcal{F}&#x3D;\{f(\mathbf{x})&#x3D;\mathbf{g}^\top\mathbf{x}: |\mathbf{g}|\le G \}$.</p>
<p>首先根据 $\mathcal{X}$，可以解出 $\mathbf{x} _\star$ 并得到：<br>$$<br>\def \x {\mathbf{x}}<br>\def \g {\mathbf{g}}<br>\def \X {\mathcal{X}}<br>\def \F {\mathcal{F}}<br>\mathcal{G} _T &#x3D; \inf _{|\x _1|\le D} \sup _{|\g _1|\le G}\cdots \inf _{|\x _T|\le D} \sup _{|\g _T|\le G} \left(\sum _{t&#x3D;1}^T \g _t^\top \x _t + D |\g _{1:T}| \right). \tag{12}<br>$$</p>
<p>根据该式推导博弈双方的策略。倒过来推，对于最后一手，环境只需考虑利用 $\mathbf{g} _T$ 最大化：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp; \langle\g _T,\mathbf{x} _T\rangle + D|\g _T + \g _{1:T-1}|\<br>&#x3D;{} &amp; \langle\g _T,\mathbf{x} _T\rangle + D\sqrt{|\g _T|^2+|\g _{1:T-1}|^2 + 2\langle \g _T ,\g _{1:T-1}\rangle}.<br>\end{align*}<br>$$<br>显然对于环境而言，最好能找到一个与 $\mathbf{x} _T$ 和 $\mathbf{g} _{1:T-1}$ 夹脚都为正的方向，然后模长拉满到 $G$。据此倒推，学习者抵抗这一策略的方法就是让 $\mathbf{x} _T$ 恰好为 $\mathbf{g} _{1:T-1}$ 的反方向。令 $\mathbf{x} _T&#x3D; -d _T \frac{\mathbf{g} _{1:T-1}}{|\mathbf{g} _{1:T-1}|}$，$d _T\ge 0$ 待定，此时再令环境通过 $\mathbf{g} _T$ 最大化：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp; \langle\g _T,\mathbf{x} _T\rangle + D\sqrt{|\g _T|^2+|\g _{1:T-1}|^2 + 2\langle \g _T ,\g _{1:T-1}\rangle} \<br>&#x3D;{} &amp; \frac{-d _T}{|\g _{1:T-1}|}\langle\g _T,\g _{1:T-1}\rangle + D\sqrt{|\g _T|^2+|\g _{1:T-1}|^2 + 2\langle \g _T ,\g _{1:T-1}\rangle} \<br>&#x3D;{} &amp; \frac{-d _T}{|\g _{1:T-1}|}a + D\sqrt{G^2+|\g _{1:T-1}|^2 + 2a}, \qquad(a\triangleq \langle \g _T ,\g _{1:T-1}\rangle)<br>\end{align*}<br>$$<br>其中我们还令 $|\mathbf{g} _T|$ 先拉到最大模长 $G$，同时也不影响 $a$ 取值。上式对 $a$ 求导：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp;\frac{-d _T}{|\g _{1:T-1}|} + \frac{D}{\sqrt{G^2+|\g _{1:T-1}|^2 + 2a}} &#x3D; 0, \<br>\implies{} &amp; 2a &#x3D; \frac{D^2}{d _T^2}|\g _{1:T-1}|^2 - G^2 - |\g _{1:T-1}|^2.<br>\end{align*}<br>$$<br>将 $a$ 代回，然后求解使最小化的 $d _T$：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp; \frac{-d _T}{|\g _{1:T-1}|}a + D\sqrt{G^2+|\g _{1:T-1}|^2 + 2a} \<br>&#x3D;{} &amp; \frac{D^2|\g _{1:T-1}|}{2d _T} + \frac{d _T(G^2 + |\g _{1:T-1}|^2)}{2|\g _{1:T-1}|} \<br>&#x3D;{} &amp; D\sqrt{G^2 + |\g _{1:T-1}|^2}. \tag*{$(d _T &#x3D; D\frac{|\g _{1:T-1}|}{\sqrt{G^2 + |\g _{1:T-1}|^2}})$}<br>\end{align*}<br>$$<br>于是我们解出了学习者的最优策略。$d _T$ 代回 $a$ 可知 $a&#x3D;0$，所以环境的最优策略是令 $\mathbf{g} _T$ 与 $\mathbf{x} _T$ 和 $\mathbf{g} _{1:T-1}$ 分别正交且模长拉满，这在维度大等于 $3$ 时始终可以做到。</p>
<p>通过数学归纳法，每一轮学习者和环境都采用该策略，于是<br>$$<br>\def \x {\mathbf{x}}<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>\mathcal{G} _T &amp;&#x3D; \inf _{|\x _1|\le D} \sup _{|\g _1|\le G}\cdots \inf _{|\x _T|\le D} \sup _{|\g _T|\le G} \left(\sum _{t&#x3D;1}^T \g _t^\top \x _t + D |\g _{1:T}| \right) \<br>&amp;&#x3D; D\sqrt{G^2 + |\g _{1:T-1}|^2} &#x3D; D\sqrt{G^2 + G^2 + |\g _{1:T-2}|^2} &#x3D; \cdots \<br>&amp;&#x3D; GD \sqrt{T}. \tag{13}<br>\end{align*}<br>$$</p>
<p>这就是该 $(\mathcal{X,F})$ setting 下对于最优算法能得到的 regret，任何算法的 regret 上界都不可能小于这个值。因此，我们之前得到的 $GD\sqrt{T}$ 已经不可能从阶上、甚至是常数上有所改进了。</p>
<p>关于 $\Omega(GD\sqrt{T})$ 的证明还有一种令在线函数随机化的 setting，虽然得到的下界比本文在常数上小一些，但是或许更加涉及本质。感兴趣的读者可以阅读<a target="_blank" rel="noopener" href="https://parameterfree.com/2019/09/25/lower-bounds-for-online-linear-optimization/">这个博客</a>的第一节。</p>
<h2 id="5-Worst-case-Adaptive-Bounds"><a href="#5-Worst-case-Adaptive-Bounds" class="headerlink" title="5. Worst-case &#x2F; Adaptive Bounds"></a>5. Worst-case &#x2F; Adaptive Bounds</h2><p>在上文中，我们已经基本了解在最经典的凸函数 setting 下使用在线梯度下降算法优化的 regret 上界，并证明从阶上无法再改进。这种上界也被称为 “worst-case bound”，也就是环境从始至终都和学习者对着干。</p>
<p>当然，还有很多情况下环境并没有这么具有对抗性，这种情况下有些算法就能够自适应地改进对应的 regret 上界，得到“adaptive bound”，它只有当环境变得完全对抗时才会退化成 worst-case 对应的情况。一个简单的例子就是式 <a href="#gradient-descent-adaptive-bound">$(9)$</a> 的最后一行，我们已经得到一个 $\mathcal{O}(D\sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\mathbf{x} _t)|^2})$ 的 adaptive bound。只有当每一轮的梯度都拉满到 $G$ 时，才会退化回 $\mathcal{O}(GD\sqrt{T})$ 的 worst-case bound；而其他情况下它可以变得更小。</p>
<p>在下一篇中，我们会改进在线梯度下降算法，让它更具适应性！我们还会发现，这类算法可以和很多领域产生关联，比如优化领域的加速方法、零和博弈里的加速求解纳什均衡，等等。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/OCO/" class="print-no-link">#OCO</a>
      
        <a href="/tags/Tutorial/" class="print-no-link">#Tutorial</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Online Convex Optimization Warmup</div>
      <div>http://example.com/2025/09/06/Online-Convex-Optimization-Warmup/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Yuheng Zhao</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>September 6, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/09/06/hello-world/" title="Hello World">
                        <span class="hidden-mobile">Hello World</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
