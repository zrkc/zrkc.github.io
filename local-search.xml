<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Online Convex Optimization Warmup</title>
    <link href="/2025/09/06/Online-Convex-Optimization-Warmup/"/>
    <url>/2025/09/06/Online-Convex-Optimization-Warmup/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文是为<a href="https://www.pengzhao-ml.com/course/AOpt2025fall/">高级优化课程</a>的学生在学习在线凸优化（Lecture 5）之前准备的。具体来说，本文从优化角度引入在线凸优化，并对于在线凸优化的一个经典 setting，介绍在线梯度下降算法和其理论保障。<br>本文的目的是希望读者能在开启正式学习之前，先对其理论推导有个大致的概览、并做一点思考探究甚至是怀疑，以免在后续学习过程中陷于一步步数学证明的验证而失去了最宝贵的兴趣。为照顾逻辑表达，本文行文叙述和理论推导可能不会十分严谨。欢迎评论！:)</p></blockquote><h2 id="0-Introduction-Why-Online-Optimization"><a href="#0-Introduction-Why-Online-Optimization" class="headerlink" title="0. Introduction: Why Online Optimization?"></a>0. Introduction: Why Online Optimization?</h2><p>本文从传统的优化角度引入，建议至少看完第 0 节后再决定是否继续看下去 :)</p><h3 id="0-1-Offline-Optimization"><a href="#0-1-Offline-Optimization" class="headerlink" title="0.1 (Offline) Optimization"></a>0.1 (Offline) Optimization</h3><p>大多数人接触优化是从随机梯度下降（Stochastic Gradient Descent, SGD）开始的，再后来就是调包创建优化器：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs py">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure><p>并训练（优化）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero _grad()<br>    output = model(<span class="hljs-built_in">input</span>)<br>    loss = loss _fn(output, target)<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure><p>截止目前（2025年），最常用的优化器是 <a href="https://arxiv.org/abs/1412.6980">Adam</a> 以及其变体 <a href="https://arxiv.org/abs/1711.05101">AdamW</a>。优化器还有很多，但是为什么现在大家几乎默认使用 Adam 呢？一个很有意思的<a href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/">观点</a>是：Adam 很适合优化若干年前的神经网络，因此神经网络近些年都在朝着能够继续使得 Adam 有效甚至越来越有效的方向进化。至于在此期间还有其他可能的新架构，往往因为诞生之初难以被<em>默认</em>的 Adam 优化而被忽视了。如果事实真就如此，我们能否从理论上说明这个现象，并指导下一个优化器（或者更可能说是，下一对“模型-优化器”）的设计呢？</p><p>我们需要回到优化的数学形式化描述上。例如，对于“feature-target”对的数据集 $\{(\mathbf{z} _n,\mathbf{y} _n)\} _{n&#x3D;1}^N$，我们定义参数为 $\mathbf{w}$ 的模型为函数 $\mathcal{M}(\cdot;\mathbf{w}):\mathbf{z}\mapsto\mathcal{M}(\mathbf{z};\mathbf{w})$。定义单值的损失函数 $l(\cdot,\cdot)$ 用以衡量模型输出 $\mathcal{M}(\mathbf{z} _n;\mathbf{w})$ 和真实目标 $\mathbf{y} _n$ 之间的差距。我们设定 $\mathbf{w}$ 属于某个参数空间的集合 $\mathcal{W}\subseteq\mathbb{R}^d$，那么优化问题可以写成：</p><p>$$<br>\begin{equation}<br>    \min _{\mathbf{w}\in\mathcal{W}}\quad  \frac{1}{N} \sum _{n&#x3D;1}^N l(\mathcal{M}(\mathbf{z} _n;\mathbf{w}),\mathbf{y} _n). \tag{1}<br>\end{equation}<br>$$</p><p>我们假设数据集是从某个分布 $\mathcal{D}$ 上独立同分布采样得到的，从而令优化问题为：</p><p>$$<br>\begin{equation}<br>    \min _{\mathbf{w}\in\mathcal{W}}\quad  \mathbb{E} _{(\mathbf{z,y})\sim\mathcal{D}}\left[ l(\mathcal{M}(\mathbf{z};\mathbf{w}),\mathbf{y})\right]. \tag{2}<br>\end{equation}<br>$$</p><p>上述问题可以更一般化：我们定义优化目标函数为 $\mathcal{L}(\mathbf{w}):\mathbf{w}\mapsto \mathbb{R}$，并假设它有一个随机 Oracle $\ell(\mathbf{w};\xi)$，满足 $\mathbb{E}_\xi[\ell(\mathbf{w};\xi)\mid\mathbf{w}] &#x3D; \mathcal{L}(\mathbf{w})$。可以<a href="https://optmlclass.github.io/notes/optforml_notes.pdf">证明</a>此时还有 $\mathbb{E}_\xi[\nabla\ell(\mathbf{w};\xi)\mid\mathbf{w}] &#x3D; \nabla\mathcal{L}(\mathbf{w})$。例如，对于某个样本 batch $\mathcal{B}&#x3D;\{(\mathbf{z} _n,\mathbf{y} _n)\}$，对应的损失 $\frac{1}{|\mathcal{B}|}\sum _{(\mathbf{z} _n,\mathbf{y} _n)\in\mathcal{B}}l(\mathcal{M}(\mathbf{z} _n;\mathbf{w}),\mathbf{y} _n)$ 可以看作是对随机 Oracle 的一次访问 $\ell(\mathbf{w};\mathcal{B})$。最终优化问题为：</p><p>$$<br>\begin{equation}<br>    \min _{\mathbf{w}\in\mathcal{W}}\quad \mathcal{L}(\mathbf{w}). \tag{3}<br>\end{equation}<br>$$</p><p>这个优化问题也被称为“随机优化（Stochastic Optimization）”。我们将最优参数记作 $\mathbf{w}^\star\triangleq \arg\min _{\mathbf{w}\in\mathcal{W}}\mathcal{L}(\mathbf{w})$。</p><p>这里我们研究优化过程的收敛性质。具体来说，假设优化过程中我们访问了 $T$ 次随机 Oracle（例如，计算了 $T$ 个 batch），优化器最终给出的模型参数为 $\mathbf{w}^\dagger$。考虑 $\mathbf{w}^\dagger$ 的损失和最优参数 $\mathbf{w}^\star$ 的差距（optimality gap），（渐进意义下）是否存在某个关于 $T$ 的函数作为上界——比如说，关于 Oracle 随机性的期望上界：<div id="optimality-gap"></div></p><p>$$<br>\begin{equation}<br>    \mathbb{E}\left[ \mathcal{L}(\mathbf{w}^\dagger) - \mathcal{L}(\mathbf{w}^\star) \right] \le \epsilon(T), \tag{4}<br>\end{equation}<br>$$</p><p>又或者高概率上界，即以至少 $(1-\delta)$ 的概率：</p><p>$$<br>\begin{equation}<br>\mathcal{L}(\mathbf{w}^\dagger) - \mathcal{L}(\mathbf{w}^\star) \le \epsilon(T,\delta). \tag{5}<br>\end{equation}<br>$$</p><p>正常来说我们期望 $\epsilon(T)$ 是随着 $T$ 递减的，常见的形式比如 $\mathcal{O}\big(\frac{1}{\sqrt{T}}\big),\mathcal{O}\big(\frac{1}{T}\big)$ 等。</p><h3 id="0-2-Offline-to-Online"><a href="#0-2-Offline-to-Online" class="headerlink" title="0.2 Offline to Online"></a>0.2 Offline to Online</h3><p>讲到这里，和我们要介绍的在线优化是什么关系呢？我们考虑模型的两个阶段：离线训练和在线部署。离线阶段，模型对着固定的数据集训练；而在线部署后，模型面对流式到来的数据，也可能有优化参数以适应新数据的需要——这方面的例子比如后训练（或者多次后训练）、on-policy 强化学习、持续学习等。为了对比和区分，我们将前者的优化过程（例如上文提到的随机优化）称为“离线优化”（Offline Optimization），而将后者称为“在线优化”（Online Optimization）。你可能会问，随机优化里一个个 batch 的数据，不也是流式的吗？——确实如此，这也是在线学习方法可以很自然地迁移到离线优化的一个原因——但离线和在线更重要的区别在于：<strong>数据分布是否会发生变化</strong>。</p><p>对于离线优化而言，优化过程中 Oracle 的分布始终不变，就比如在随机优化中，每个 batch 都是<strong>独立同分布</strong>的。事实上，当数据集被扩充、或者进入在线部署阶段后，数据分布很可能会发生变化，始终独立同分布的条件就不成立了。此时，优化问题从“离线”的一成不变转为“在线”的持续变化，“在线优化”也就有了意义。</p><p>等等，在上一小节我们不是说好了要研究优化器的理论吗？这饼给我画哪去了？事实上，正如上所述，在线优化的方法可以很自然地用于离线优化尤其是随机优化，原因也很简单：在线学习研究的是分布可以发生变化，它的一个特殊情况就是分布不变的随机优化。而从理论上，也有一个很简单直接的转化，称为<a href="https://parameterfree.com/2019/09/17/more-online-to-batch-examples-and-strong-convexity/">“online-to-batch conversion”</a>，它的意思是通过为离线优化设计一种黑盒调用在线优化算法的框架，式 <a href="#optimality-gap">$(4)$</a> 中的 optimality gap 可以被平均意义下的在线优化指标——遗憾（Regret）——给 upper bound 住，从而我们的问题转变为了为 Regret 提供理论保障。</p><p>值得注意的是，现在风生水起的 <a href="https://arxiv.org/abs/1412.6980">Adam (2014)</a> 优化器，在论文中就是从在线凸优化角度给出收敛性证明的（虽然后续被指出证明存在错误…存在反例使得 Adam 不收敛…）；更值得注意的是，Adam 算法的设计结合了 1) <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36483.pdf">AdaGrad (2010)</a> 的变体 <a href="https://arxiv.org/pdf/1308.0850v5.pdf">RMSProp (2014)</a> 和 2) 动量方法（早已有之，而最重要的工作是 Yurii Nesterov 在 1983 年提出的 <a href="https://link.springer.com/book/10.1007/978-3-319-91578-4">Nesterov Accelerated Gradient</a>，大幅加速了梯度下降的收敛率）。AdaGrad 作为在线凸优化和随机优化算法，将在线凸优化中一个很重要的步长设计 <a href="https://arxiv.org/abs/1002.4862">adaptive learning rate (2010)</a>（本文后面会介绍）直接迁移到 entry-wise，从而更好地利用梯度的稀疏性。总之，Adam(W) 上溯祖宗三代都有在线凸优化——虽然它如今已经成为了大模型时代的默认优化器（而且还是<strong>非凸</strong>的…）——是因为 Adam 恰好成为了大模型时代爆发前夜被选中的“<a href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/">幸运儿</a>”，还是说在线凸优化有着超出我们当前认知的潜力呢？</p><p>在线优化所属的在线学习（Online Learning），最早由<a href="https://www.researchgate.net/profile/Gabor-Lugosi/publication/220690817_Prediction_Learning_and_Games/links/0912f50eae7fc7be04000000/Prediction-Learning-and-Games.pdf">《Prediction, learning, and games》</a>一书从博弈论的角度所形式化，并给出了 Regret 概念作为理论指标。当然，作为研究分布变化情况下的优化问题，在线优化还具备更广泛的意义和应用场景。没有一成不变的环境，为了得到有适应能力的模型，在线优化也算是起了一个开头了。</p><h2 id="1-Problem-Formulation"><a href="#1-Problem-Formulation" class="headerlink" title="1. Problem Formulation"></a>1. Problem Formulation</h2><p>在线优化属于在线学习（Online Learning）。在线学习简单来说就是设计与环境交互的算法，根据得到的反馈不断更新自身模型以适应。我们将这个过程抽象成学习者与环境的<strong>博弈</strong>（实际上“博弈”这个词有一定误导性，因为环境不一定总是对抗性的），即如下范式：</p><div style="border: 1px solid rgba(127, 127, 127, 1); padding: 10px; border-radius: 5px;"><p><strong>在线学习.</strong><br>已知决策可行域 $\mathcal{X}\subseteq\mathbb{R}^d$。在第 $t&#x3D;1,2,\dots$ 轮：</p><ol><li>学习者提交决策 $\mathbf{x} _t\in\mathcal{X}$；</li><li>环境选择在线函数 $f _t:\mathcal{X}\to\mathbb{R}$；</li><li>学习者受到 $f _t(\mathbf{x} _t)$ 的损失，观测到某些 $f _t$ 的信息并更新决策。</div></li></ol><p>在线凸优化（Online Convex Optimization, OCO）研究可行域 $\mathcal{X}$ 为凸集且在线函数 $f _t$ 均为凸函数的情况。</p><h3 id="1-1-Performance-Measure"><a href="#1-1-Performance-Measure" class="headerlink" title="1.1 Performance Measure"></a>1.1 Performance Measure</h3><p>博弈双方的指标是什么呢？最直接地，我们希望学习者能最小化累积损失：</p><p>$$<br>\def \x {\mathbf{x}}<br>\sum _{t&#x3D;1}^T f _t(\x _t). \tag{6}<br>$$</p><p>但这个指标对于学习者来说很不公平，因为环境可以对抗性地根据 $\mathbf{x} _t$ 给出 $f _t$，导致很难为该指标给出有意义的保障。类似式 <a href="#optimality-gap">$(4)$</a> 的 optimality gap（也称为超额风险）引入最优参数作为 comparator，这里我们引入<strong>全局最优固定决策</strong> $\mathbf{x} _\star\triangleq \arg\min _{\mathbf{x}\in\mathcal{X}}\sum _{t&#x3D;1}^T f _t(\mathbf{x})$ 作为学习者的 comparator，得到称为“遗憾”（Regret）的指标：<div id="regret"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>Reg _T \triangleq \sum _{t&#x3D;1}^T f _t(\x _t) - \min _{\x\in\mathcal{X}}\sum _{t&#x3D;1}^T f _t(\x). \tag{7}<br>$$</p><p>学习者的目标是最小化遗憾。由于环境的决策此时也会影响 $\mathbf{x} _\star$ 的累积损失，因此这是一个更合理的博弈指标。</p><p>在本文中，我们还同时关注两个 OCO 中比较常见的假设：可行域有界假设、梯度有界假设。</p><div style="border: 1px solid rgba(127, 127, 127, 1); padding: 10px; border-radius: 5px;"><ul><li><strong>可行域有界假设.</strong> 对任意 $\mathbf{x,y}\in\mathcal{X}$，有 $|\mathbf{x-y}| _2\le D$.  </li><li><strong>梯度有界假设.</strong> 对任意 $\mathbf{x}\in\mathcal{X}$ 和 $t\in[T]$，有 $|\nabla f _t(\mathbf{x})| _2\le G$。即在线函数是 $G$-Lipschitz 的。</li></ul></div><p>下文简记 $\ell _2$-范数 $|\cdot| _2$ 为 $|\cdot|$。引入这两个假设最直接的作用就是简化了算法设计和证明。这两个假设也很基本，从量纲上来说分别对应了距离和斜率，乘起来就是损失。当然也有很多工作在研究不依赖这两个假设或者对应参数的算法，至今仍有很多有待研究的空间。</p><p>到这里，我们也可以解释一下上文提到的“”</p><h3 id="1-2-A-Trivial-Example"><a href="#1-2-A-Trivial-Example" class="headerlink" title="1.2 A Trivial Example"></a>1.2 A Trivial Example</h3><p>下文中，我们要做的就是设计算法，并证明 regret 有上界保证——通常用渐进复杂度上界表示——并且至少得是关于 $T$ 亚线性的。为什么会这么考虑呢？首先，亚线性意味着平均意义下相对于最优决策的损失之差会趋于零：$\lim _{T\to\infty}\frac{Reg _T}{T}&#x3D;0$，从而累积损失趋近于全局最优决策 $\mathbf{x} _\star$ 的累计损失。其次，一个关于 $T$ 线性的上界是 trivial 的。最简单地，根据微分中值定理：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \c {\mathbf{c}}<br>\begin{align*}<br>Reg _T &amp;&#x3D; \sum _{t&#x3D;1}^T f _t(\x _t) - \sum _{t&#x3D;1}^T f _t(\xs) &#x3D; \sum _{t&#x3D;1}^T \langle \nabla f _t(\c _t), \x _t - \xs \rangle \qquad(\c _t\in[\x _t,\xs]) \<br>&amp;\le \sum _{t&#x3D;1}^T | \nabla f _t(\c _t) | |\x _t - \xs| \le GDT.<br>\end{align*}<br>$$<br>其中第一个不等号使用了 Hölder 不等式。我们可以稍微观察一下这个上界：$GD$ 是最大“斜率”乘最大“距离”，也就是每轮可能的最大损失，这个损失累积了 $T$ 轮，所以 $GDT$ 就是 regret 可能的最大值，是非常 trivial 的。</p><p>推导上界一定得是一个有意义的上界✍️。在后文中，我们会最终得到一个亚线性的 $\mathcal{O}(GD\sqrt{T})$ 上界，并证明（在某种意义上）无法做到更优。</p><p>本节的最后，也是在很多经典文献中证明 regret 上界的第一步，就是做线性化：即利用 $f _t(\mathbf{x})$ 为凸函数这一假设：</p><div id="regret-linearized"></div><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>Reg _T &#x3D; \sum _{t&#x3D;1}^T f _t(\x _t) - \sum _{t&#x3D;1}^T f _t(\xs) \le \sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs \rangle. \tag{2}<br>$$</p><p>相比刚才的微分中值定理，好处是我们不需要知道那个中值点 $\mathbf{c} _t$，坏处也很显然，假设函数为凸本身就局限了它的应用。无论如何，接下来可以设计我们的第一个在线凸优化算法了！</p><h2 id="2-Online-Gradient-Descent"><a href="#2-Online-Gradient-Descent" class="headerlink" title="2. Online Gradient Descent"></a>2. Online Gradient Descent</h2><h3 id="2-1-Why-Online-Gradient-Descent"><a href="#2-1-Why-Online-Gradient-Descent" class="headerlink" title="2.1 Why Online Gradient Descent?"></a>2.1 Why Online Gradient Descent?</h3><p>OCO 问题最经典的算法框架之一是在线梯度下降（Online Gradient Descent, OGD），即<br>$$<br>\def \x {\mathbf{x}}<br>\def \xt {\widetilde{\x}}<br>\xt _{t+1} &#x3D; \x _t - \eta _t \nabla f _t(\x _t),\quad \x _{t+1} &#x3D; \Pi _{\mathcal{X}}[ \xt _{t+1} ],<br>$$<br>其中 $\eta _t&gt;0$ 是步长，$\Pi _{\mathcal{X}}[\cdot]$ 是向凸集 $\mathcal{X}$ 的投影操作。可能你立即会问了，为什么：一定得从上一步做更新而不是上上步甚至随便一个点？从博弈过程来看，$f _t$ 和 $f _{t+1}$ 似乎没什么联系，那么从 $\mathbf{x} _t$ 拿着 $f _t$ 在该点的梯度去更新得到 $\mathbf{x} _{t+1}$，对于优化 $f _{t+1}$ 能有多大帮助呢？</p><p>这里主要解释直观上为什么 OGD 能 work。我们先抛开 OCO 的语义，从数学角度看这个问题。在后文中我们会经常见到一类数学结构，它们看起来非常类似余弦定理：<br>$$<br>\def \a {\mathbf{a}}<br>\def \b {\mathbf{b}}<br>\def \c {\mathbf{c}}<br>2\langle \a - \b, \a - \c \rangle &#x3D; |\a - \b|^2 + |\a - \c|^2 - |\b - \c|^2.<br>$$</p><p>我们观察式 <a href="#regret-linearized">$(2)$</a> 线性化后的 regret：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs\rangle,<br>$$</p><p>同样是内积式。如果需要将 regret 和余弦定理联系起来，一个思路是观察到向余弦定理代入：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>2\langle \x _t - \x _{t+1}, \x _t - \xs \rangle &#x3D; |\x _t - \x _{t+1}|^2 + |\x _t - \xs|^2 - |\x _{t+1} - \xs|^2.<br>$$</p><p>这个内积式目前还不依赖任何 $\mathbf{x} _t$ 和 $\mathbf{x} _{t+1}$ 之间的关系，也就是适用于任意序列 $\{\mathbf{x} _t\} _{t&#x3D;1}^T$。对 $t&#x3D;1$ 到 $T$ 求和，会发现它可以<strong>错位相消</strong>（telescoping）：</p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>2\sum _{t&#x3D;1}^T \langle \x _t - \x _{t+1}, \x _t - \xs \rangle &#x3D; \sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2 + |\x _1 - \xs|^2 - |\x _{T+1} - \xs|^2.<br>$$</p><p>能做到错位相消是一个我们比较满意的结构。为了让它和 regret 沾上边，对比二者形式我们发现最好让 $\mathbf{x} _t-\mathbf{x} _{t+1}$ 和 $\nabla f _t(\mathbf{x} _t)$ 对应。当然我们还可以引入步长 $\eta$ 作为算法的参数。如果不考虑投影操作，最简单的设计就是 $\mathbf{x} _t-\mathbf{x} _{t+1}&#x3D;\eta\nabla f _t(\mathbf{x} _t)$，它已经很类似在线梯度下降的更新式了。代入 regret 可得：<div id="regret-telescoping"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>Reg _T &amp;\le \sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs \rangle &#x3D; \frac{1}{\eta}\sum _{t&#x3D;1}^T \langle \x _t - \x _{t+1}, \x _t - \xs \rangle \<br>&amp;&#x3D; \frac{1}{2\eta}\left(\sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2 + |\x _1 - \xs|^2 - |\x _{T+1} - \xs|^2. \right) \<br>&amp;&#x3D; \frac{\eta}{2}\sum _{t&#x3D;1}^T |\nabla f _t(\x _t)|^2 + \frac{1}{2\eta}\left(|\x _1 - \xs|^2 - |\x _{T+1} - \xs|^2\right). \tag{3}<br>\end{align*}<br>$$</p><p>令人高兴的是，我们可以进一步利用 $\eta$ 在上式做 trade-off！可以大概预想到，通过合理设置 $\eta$，我们能得到一个亚线性，具体而言，$\mathcal{O}(\sqrt{T})$ 的上界。</p><p>在阅读下一小节之前，你可能会有一些自己的想法。你可以试试自己推导看看，这会很快帮助你形成一个比较合理的认知 :)</p><h3 id="2-2-Some-Questions"><a href="#2-2-Some-Questions" class="headerlink" title="2.2 Some Questions"></a>2.2 Some Questions</h3><p>到这里，不知道你会不会问：为什么不做“梯度上升”呢？从分析上看似乎也有些道理：上面的余弦定理是两个正项和一个负项，梯度上升意味着使用余弦定理取负，变成两个负项一个正项，而且依然保留 telescoping 的结构！但是，不能这么做的原因在于，那个原本为负、现在为正的项，最终会变得难以控制地大。</p><p>例如，令 $\mathbf{x} _t-\mathbf{x} _{t+1}&#x3D;-\eta\nabla f _t(\mathbf{x} _t)$，代入：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\eta\langle \nabla f _t(\x _t), \x _t - \xs\rangle &#x3D; -<br>\langle \x _t - \x _{t+1}, \x _t - \xs \rangle &#x3D; \frac{1}{2}\Big(|\x _{t+1} - \xs|^2 - |\x _t - \xs|^2 - |\x _t - \x _{t+1}|^2\Big).<br>$$</p><p>对 $t&#x3D;1$ 到 $T$ 求和（注意我们忽略了投影）：<div id="gradient-ascent"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>Reg _T \le \frac{1}{2\eta}\left(|\x _{T+1} - \xs|^2 - |\x _1 - \xs|^2 - \sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2\right) . \tag{4}<br>$$</p><p>看起来只有唯一的正项 $|\mathbf{x} _{T+1}-\mathbf{x} _\star|^2$，莫非这是一个好事？但这还没完。式 <a href="#regret-telescoping">$(3)$</a> 中的正项 $|\mathbf{x} _1 - \mathbf{x} _\star|^2$，通常来说，我们会认为它是一个和初始点选择有关的常数；而式 <a href="#gradient-ascent">$(4)$</a> 中的正项 $|\mathbf{x} _{T+1}-\mathbf{x} _\star|^2$ 就是一个和算法运行过程相关的量了。因为推导忽略了投影，相当于令 $\mathcal{X}&#x3D;\mathbb{R}^d$，所以此时可行域有界假设不成立；万一 $|\mathbf{x} _{T+1}-\mathbf{x} _\star|^2$ 随着 $T$ 增长速度是线性的呢？这也体现了证明上界的一个基本原则：虽然证明过程中产生了很多大等于 regret 的式子，但是只有当这个式子有普适意义时（例如，和算法运行过程量无关，体现为某些全局参数的函数），才能够和其他算法对比。下一步，我们顶多利用 Cauchy–Schwarz 不等式：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>&amp; |\x _{T+1} - \xs|^2 &#x3D; |\x _{T+1}-\x _T+\x _T - \x _{T-1} + \cdots + \x _2 - \x _1 + \x _1 - \xs|^2 \<br>\le{} &amp; (T+1)\left(|\x _{T+1}-\x _T|^2 + |\x _T - \x _{T-1}|^2 + \cdots + |\x _2 - \x _1|^2 + |\x _1 - \xs|^2 \right),<br>\end{align*}<br>$$</p><p>代入式 <a href="#gradient-ascent">$(4)$</a> 得到：<div id="gradient-ascent-regret"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>Reg _T &amp;\le \frac{T}{2\eta}\left( |\x _1 - \xs|^2 + \sum _{t&#x3D;1}^T |\x _t - \x _{t+1}|^2 \right) &#x3D; \frac{T}{2}\left( \frac{1}{\eta}|\x _1 - \xs|^2 + \eta\sum _{t&#x3D;1}^T |\nabla f _t(\x _t)|^2 \right). \tag{5}<br>\end{align*}<br>$$</p><p>很显然，和梯度下降的式 <a href="#regret-telescoping">$(3)$</a> 相比，式 <a href="#gradient-ascent-regret">$(5)$</a> 中 $\eta$ 最多只能在括号里头做 trade-off，而括号外还有一个线性的 $T$，高下立判！</p><p>总而言之，在线梯度下降能够 work 就是因为它合理地利用了余弦定理形式的 telescoping 结构。你可能还会问：为什么得从上一步而不是上上步或者其他呢？从上面的推导可以看出，从上一步只是为了方便做 telescoping 的衔接。事实上，不一定得从上一步开始，只要你能构造出一条（或几条）的链条就行了，比如：<br>$$<br>\def \x {\mathbf{x}}<br>\x _1 \underset{-\nabla f _1(\x _1)}{\longrightarrow} \x _3 \underset{-\nabla f _3(\x _3)}{\longrightarrow} \cdots,\quad \x _2 \underset{-\nabla f _2(\x _2)}{\longrightarrow} \x _4 \underset{-\nabla f _4(\x _4)}{\longrightarrow} \cdots.<br>$$<br>更夸张地，你还可以：<br>$$<br>\def \x {\mathbf{x}}<br>\x _1 \underset{-\nabla f _1(\x _1)}{\longrightarrow} \x _T ,\quad \x _2 \underset{-\nabla f _2(\x _2)}{\longrightarrow} \x _3 \underset{-\nabla f _3(\x _3)}{\longrightarrow} \cdots \underset{-\nabla f _{T-2}(\x _{T-2})}{\longrightarrow} \x _{T-1}.<br>$$</p><p>不过其实就相当于把在线函数分为两组，跑两个 OGD 算法，本质上还是脱离不开“从上一步更新”的框架。能做成 telescoping 的关键还在于要从 $\mathbf{x} _t$ 用这点的梯度信息 $\nabla f _t(\mathbf{x} _t)$ 更新，剩下的就是一些变体了。</p><p>下一节中，我们对在线梯度下降进行正式的理论推导，并得到第一个亚线性的上界。</p><h2 id="3-Regret-Analysis"><a href="#3-Regret-Analysis" class="headerlink" title="3. Regret Analysis"></a>3. Regret Analysis</h2><h3 id="3-1-Bias-Variance"><a href="#3-1-Bias-Variance" class="headerlink" title="3.1 Bias-Variance"></a>3.1 Bias-Variance</h3><p>回到在线梯度下降：<div id="gradient-descent"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xt {\widetilde{\x}}<br>\xt _{t+1} &#x3D; \x _t - \eta _t \nabla f _t(\x _t),\quad \x _{t+1} &#x3D; \Pi _{\mathcal{X}}[ \xt _{t+1} ]. \tag{6}<br>$$</p><p>虽然看起来多了一步投影，但是和上面的推导基本一样：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \xt {\widetilde{\x}}<br>\begin{align*}<br>Reg _T &amp;\le \sum _{t&#x3D;1}^T \langle \nabla f _t(\x _t), \x _t - \xs\rangle &#x3D; \sum _{t&#x3D;1}^T \frac{1}{\eta _t}\langle \x _t - \xt _{t+1}, \x _t - \xs\rangle \<br>&amp;&#x3D; \sum _{t&#x3D;1}^T \frac{1}{2\eta _t} \Big(| \x _t - \xt _{t+1} |^2 + |\x _t - \xs|^2 - |\xt _{t+1} - \xs|^2 \Big).<br>\end{align*}<br>$$</p><p>为了 telescoping，我们利用投影的性质（毕达哥拉斯定理）做一步放缩：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \y {\mathbf{y}}<br>\def \xt {\widetilde{\x}}<br>\x &#x3D; \Pi _{\mathcal{X}}[ \xt ],\quad\implies\quad \forall \y\in\mathcal{X},|\x-\y|\le |\xt - \y|.<br>$$</p><p>于是上式：<div id="gradient-descent-regret"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\def \xt {\widetilde{\x}}<br>\begin{align*}<br>Reg _T &amp;\le \sum _{t&#x3D;1}^T \frac{1}{2\eta _t} \Big(| \x _t - \xt _{t+1} |^2 + |\x _t - \xs|^2 - |\x _{t+1} - \xs|^2 \Big) \<br>&amp;&#x3D; \frac{1}{2}\sum _{t&#x3D;1}^T \eta _t| \nabla f _t(\x _t) |^2 + \sum _{t&#x3D;1}^T \frac{1}{2\eta _t}\Big(|\x _t - \xs|^2 - |\x _{t+1} - \xs|^2 \Big). \tag{7}<br>\end{align*}<br>$$</p><p>该式子可以看出有两部分：第一部分梯度范数累积正比于步长，第二部分 telescoping 反比于步长。大致理解来看，我们可以认为第一部分是算法的 variance；第二部分是算法的 bias（这个含义在下一篇文章中更加明确地拆解出来，挖坑）。它也体现了一个在线学习算法永远在做的一件事：在探索与利用之间做平衡——每一步更新既要根据新信息做及时的更新，也要充分利用已有信息——平衡的关键就在于步长的设计。对于在线梯度下降来说，从上一步用梯度走多远，就是探索与利用的过程。这个思想在很多领域都有所体现。</p><p>在下一篇文章中，我们也会尝试更加显式地拆分出 trade-off 的两部分。这个拆分也将引出一类更加具有自适应能力的在线学习算法，并和更广泛的领域产生联系。</p><p>总之，我们先尝试设计步长以 trade-off。</p><h3 id="3-2-Step-size-Design"><a href="#3-2-Step-size-Design" class="headerlink" title="3.2 Step-size Design"></a>3.2 Step-size Design</h3><p>先考虑简单情况。如果 $\eta _t\equiv \eta$，则式 <a href="#gradient-descent-regret">$(7)$</a>：<br>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>Reg _T \le \frac{\eta}{2} \sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2 + \frac{1}{2\eta}|\x _1 - \xs|^2 \le \frac{\eta G^2 T}{2} + \frac{D^2}{2\eta} &#x3D; GD\sqrt{T}. \tag{8}<br>$$<br>其中我们令 $\eta&#x3D;\frac{D}{G\sqrt{T}}$。</p><p>这里还可以考虑变步长 $\eta _t$，得到一个更有意思的设计。具体需要利用 Self-confident Tuning Lemma 这一数学工具，假设 $a  _1 ,\dots, a  _T$ 是非负实数，则：<br>$$<br>\sqrt{\sum _{t&#x3D;1}^T a _t} \le \sum _{t&#x3D;1}^T\frac{a _t}{\sqrt{\sum _{s&#x3D;1}^t a _s}}\le 2\sqrt{\sum _{t&#x3D;1}^T a _t}.<br>$$<br>现在参照上述引理，我们使用变步长并定义为 $\eta _t&#x3D;\frac{\alpha}{\sqrt{\sum _{s&#x3D;1}^t|\nabla f _s(\mathbf{x} _s)|^2}}$（其也称为 “adaptive step-size”，$\alpha&gt;0$ 为待定常数）。回到式 <a href="#gradient-descent-regret">$(7)$</a>：<div id="gradient-descent-adaptive-bound"></div></p><p>$$<br>\def \x {\mathbf{x}}<br>\def \xs {\x _\star}<br>\begin{align*}<br>Reg _T &amp;\le \frac{1}{2}\sum _{t&#x3D;1}^T \eta _t| \nabla f _t(\x _t) |^2 + \sum _{t&#x3D;2}^T \left(\frac{1}{2\eta _t} - \frac{1}{2\eta _{t-1}}\right) |\x _t - \xs|^2 + \frac{1}{2\eta _1}|\x _1 - \xs|^2 \<br>&amp;\le \frac{1}{2}\sum _{t&#x3D;1}^T \eta _t| \nabla f _t(\x _t) |^2 + \frac{D^2}{2\eta _T} &#x3D; \frac{\alpha}{2}\sum _{t&#x3D;1}^T \frac{|\nabla f _t(\x _t)|^2}{\sqrt{\sum _{s&#x3D;1}^t|\nabla f _s(\x _s)|^2}} + \frac{D^2\sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2}}{2\alpha} \<br>&amp;\le \left(\alpha + \frac{D^2}{2\alpha}\right) \sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2} &#x3D; \mathcal{O}\left( D\sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\x _t)|^2}\right) &#x3D; \mathcal{O}(GD\sqrt{T}), \tag{9}<br>\end{align*}<br>$$<br>其中我们令 $\alpha&#x3D;D$。相较于 $\eta&#x3D;\frac{D}{G\sqrt{T}}$，这里变步长的好处是算法只需要事先知道 $D$——由于投影操作显式地需要可行域，这一般是已知的。不需要 $T$ 意味着算法可以运行任意轮数，这也称为 “anytime” 算法。</p><h2 id="4-Lower-Bound"><a href="#4-Lower-Bound" class="headerlink" title="4. Lower Bound"></a>4. Lower Bound</h2><p>回顾证明，我们主要在两个地方使用了不等号：第一处是线性化，第二处是利用投影性质做的放缩。这两个放缩会不会导致我们的上界 $\mathcal{O}(GD\sqrt{T})$ 太松了呢？本节将会说明，对于“<strong>最坏情况</strong>”，它是紧的。</p><p>由于我们证明的上界对于所有 OCO setting 都成立，为了说明没有更优的算法或者分析方法能够进一步从阶上改进这个上界，就需要证明对于任意算法，总存在一个最坏情况使得该算法具有 $\Omega(GD\sqrt{T})$ 的 regret 下界。也就是说，我们尝试证明：</p><p>$$<br>\def \x {\mathbf{x}}<br>\inf _{\text{algorithm}} \sup _{\text{environment}} Reg _T \ge \Omega(GD\sqrt{T}). \tag{10}<br>$$</p><p>本文采用的证明思路为，当确定可行域 $\mathcal{X}$ 和在线函数集 $\mathcal{F}$ 后，我们可以直接求解以 regret 为博弈指标的 minimax 值：<br>$$<br>\def \x {\mathbf{x}}<br>\def \X {\mathcal{X}}<br>\def \F {\mathcal{F}}<br>\mathcal{G} _T\triangleq \inf _{\x _1\in\X} \sup _{f _1\in\F} \inf _{\x _2\in\X} \sup _{f _2\in\F} \cdots \inf _{\x _T\in\X} \sup _{f _T\in\F} \left(\sum _{t&#x3D;1}^T f _t(\x _t) - \min _{\x\in\X}\sum _{t&#x3D;1}^T f _t(\x) \right). \tag{11}<br>$$</p><p>所以我们只需要构造出一组 $(\mathcal{X,F})$，并证明此时 $\mathcal{G} _T\ge \Omega(GD\sqrt{T})$ 即可。我们考虑一个简单（但是依然不容易）的 case：令 $\mathcal{X}&#x3D;\{\mathbf{x}:|\mathbf{x}|\le D\}$，以及 $\mathcal{F}&#x3D;\{f(\mathbf{x})&#x3D;\mathbf{g}^\top\mathbf{x}: |\mathbf{g}|\le G \}$.</p><p>首先根据 $\mathcal{X}$，可以解出 $\mathbf{x} _\star$ 并得到：<br>$$<br>\def \x {\mathbf{x}}<br>\def \g {\mathbf{g}}<br>\def \X {\mathcal{X}}<br>\def \F {\mathcal{F}}<br>\mathcal{G} _T &#x3D; \inf _{|\x _1|\le D} \sup _{|\g _1|\le G}\cdots \inf _{|\x _T|\le D} \sup _{|\g _T|\le G} \left(\sum _{t&#x3D;1}^T \g _t^\top \x _t + D |\g _{1:T}| \right). \tag{12}<br>$$</p><p>根据该式推导博弈双方的策略。倒过来推，对于最后一手，环境只需考虑利用 $\mathbf{g} _T$ 最大化：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp; \langle\g _T,\mathbf{x} _T\rangle + D|\g _T + \g _{1:T-1}|\<br>&#x3D;{} &amp; \langle\g _T,\mathbf{x} _T\rangle + D\sqrt{|\g _T|^2+|\g _{1:T-1}|^2 + 2\langle \g _T ,\g _{1:T-1}\rangle}.<br>\end{align*}<br>$$<br>显然对于环境而言，最好能找到一个与 $\mathbf{x} _T$ 和 $\mathbf{g} _{1:T-1}$ 夹脚都为正的方向，然后模长拉满到 $G$。据此倒推，学习者抵抗这一策略的方法就是让 $\mathbf{x} _T$ 恰好为 $\mathbf{g} _{1:T-1}$ 的反方向。令 $\mathbf{x} _T&#x3D; -d _T \frac{\mathbf{g} _{1:T-1}}{|\mathbf{g} _{1:T-1}|}$，$d _T\ge 0$ 待定，此时再令环境通过 $\mathbf{g} _T$ 最大化：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp; \langle\g _T,\mathbf{x} _T\rangle + D\sqrt{|\g _T|^2+|\g _{1:T-1}|^2 + 2\langle \g _T ,\g _{1:T-1}\rangle} \<br>&#x3D;{} &amp; \frac{-d _T}{|\g _{1:T-1}|}\langle\g _T,\g _{1:T-1}\rangle + D\sqrt{|\g _T|^2+|\g _{1:T-1}|^2 + 2\langle \g _T ,\g _{1:T-1}\rangle} \<br>&#x3D;{} &amp; \frac{-d _T}{|\g _{1:T-1}|}a + D\sqrt{G^2+|\g _{1:T-1}|^2 + 2a}, \qquad(a\triangleq \langle \g _T ,\g _{1:T-1}\rangle)<br>\end{align*}<br>$$<br>其中我们还令 $|\mathbf{g} _T|$ 先拉到最大模长 $G$，同时也不影响 $a$ 取值。上式对 $a$ 求导：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp;\frac{-d _T}{|\g _{1:T-1}|} + \frac{D}{\sqrt{G^2+|\g _{1:T-1}|^2 + 2a}} &#x3D; 0, \<br>\implies{} &amp; 2a &#x3D; \frac{D^2}{d _T^2}|\g _{1:T-1}|^2 - G^2 - |\g _{1:T-1}|^2.<br>\end{align*}<br>$$<br>将 $a$ 代回，然后求解使最小化的 $d _T$：<br>$$<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>&amp; \frac{-d _T}{|\g _{1:T-1}|}a + D\sqrt{G^2+|\g _{1:T-1}|^2 + 2a} \<br>&#x3D;{} &amp; \frac{D^2|\g _{1:T-1}|}{2d _T} + \frac{d _T(G^2 + |\g _{1:T-1}|^2)}{2|\g _{1:T-1}|} \<br>&#x3D;{} &amp; D\sqrt{G^2 + |\g _{1:T-1}|^2}. \tag*{$(d _T &#x3D; D\frac{|\g _{1:T-1}|}{\sqrt{G^2 + |\g _{1:T-1}|^2}})$}<br>\end{align*}<br>$$<br>于是我们解出了学习者的最优策略。$d _T$ 代回 $a$ 可知 $a&#x3D;0$，所以环境的最优策略是令 $\mathbf{g} _T$ 与 $\mathbf{x} _T$ 和 $\mathbf{g} _{1:T-1}$ 分别正交且模长拉满，这在维度大等于 $3$ 时始终可以做到。</p><p>通过数学归纳法，每一轮学习者和环境都采用该策略，于是<br>$$<br>\def \x {\mathbf{x}}<br>\def \g {\mathbf{g}}<br>\begin{align*}<br>\mathcal{G} _T &amp;&#x3D; \inf _{|\x _1|\le D} \sup _{|\g _1|\le G}\cdots \inf _{|\x _T|\le D} \sup _{|\g _T|\le G} \left(\sum _{t&#x3D;1}^T \g _t^\top \x _t + D |\g _{1:T}| \right) \<br>&amp;&#x3D; D\sqrt{G^2 + |\g _{1:T-1}|^2} &#x3D; D\sqrt{G^2 + G^2 + |\g _{1:T-2}|^2} &#x3D; \cdots \<br>&amp;&#x3D; GD \sqrt{T}. \tag{13}<br>\end{align*}<br>$$</p><p>这就是该 $(\mathcal{X,F})$ setting 下对于最优算法能得到的 regret，任何算法的 regret 上界都不可能小于这个值。因此，我们之前得到的 $GD\sqrt{T}$ 已经不可能从阶上、甚至是常数上有所改进了。</p><p>关于 $\Omega(GD\sqrt{T})$ 的证明还有一种令在线函数随机化的 setting，虽然得到的下界比本文在常数上小一些，但是或许更加涉及本质。感兴趣的读者可以阅读<a href="https://parameterfree.com/2019/09/25/lower-bounds-for-online-linear-optimization/">这个博客</a>的第一节。</p><h2 id="5-Worst-case-Adaptive-Bounds"><a href="#5-Worst-case-Adaptive-Bounds" class="headerlink" title="5. Worst-case &#x2F; Adaptive Bounds"></a>5. Worst-case &#x2F; Adaptive Bounds</h2><p>在上文中，我们已经基本了解在最经典的凸函数 setting 下使用在线梯度下降算法优化的 regret 上界，并证明从阶上无法再改进。这种上界也被称为 “worst-case bound”，也就是环境从始至终都和学习者对着干。</p><p>当然，还有很多情况下环境并没有这么具有对抗性，这种情况下有些算法就能够自适应地改进对应的 regret 上界，得到“adaptive bound”，它只有当环境变得完全对抗时才会退化成 worst-case 对应的情况。一个简单的例子就是式 <a href="#gradient-descent-adaptive-bound">$(9)$</a> 的最后一行，我们已经得到一个 $\mathcal{O}(D\sqrt{\sum _{t&#x3D;1}^T|\nabla f _t(\mathbf{x} _t)|^2})$ 的 adaptive bound。只有当每一轮的梯度都拉满到 $G$ 时，才会退化回 $\mathcal{O}(GD\sqrt{T})$ 的 worst-case bound；而其他情况下它可以变得更小。</p><p>在下一篇中，我们会改进在线梯度下降算法，让它更具适应性！我们还会发现，这类算法可以和很多领域产生关联，比如优化领域的加速方法、零和博弈里的加速求解纳什均衡，等等。</p>]]></content>
    
    
    
    <tags>
      
      <tag>OCO</tag>
      
      <tag>Tutorial</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/09/06/hello-world/"/>
    <url>/2025/09/06/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="Additional-Information"><a href="#Additional-Information" class="headerlink" title="Additional Information"></a>Additional Information</h2><p>This blog is deployed in this <a href="https://github.com/zrkc/zrkc.github.io/tree/main">Github repo</a>, with source code stored in the branch <a href="https://github.com/zrkc/zrkc.github.io/tree/hexo">hexo</a>.</p><p>To maintain the blog in a new device, clone the branch <a href="https://github.com/zrkc/zrkc.github.io/tree/hexo">hexo</a> and run hexo:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">git clone -<span class="hljs-selector-tag">b</span> hexo https:<span class="hljs-comment">//github.com/zrkc/zrkc.github.io.git</span><br>cd .\zrkc<span class="hljs-selector-class">.github</span>.io\<br>npm install -<span class="hljs-selector-tag">g</span> hexo-cli<br>npm install<br>npm install hexo-deployer-git<br></code></pre></td></tr></table></figure><p>(And delete <code>.deploy_git</code> if downloaded from remote, this is only used local. I have added it to <code>.gitignore</code>)</p><p>To syn from local to remote (hexo branch, not main!):</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">git</span> <span class="hljs-keyword">add</span> .<br><span class="hljs-symbol">git</span> commit -m <span class="hljs-string">&quot;some comments&quot;</span><br><span class="hljs-symbol">git</span> <span class="hljs-keyword">push</span> origin hexo<br></code></pre></td></tr></table></figure><p>To syn from remote to local:</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">git pull</span><br></code></pre></td></tr></table></figure><p><a href="https://cloud.tencent.com/developer/article/1046404">reference</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
